{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put X and Y together and save as a CSV file for easier manulation in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(name):\n",
    "    \"\"\"\n",
    "    Consolidate X, y and save to a csv file.\n",
    "    \n",
    "    Params:\n",
    "        name: string, name of the data file name excluding extension.\n",
    "    \n",
    "    \"\"\"\n",
    "    def read_eval(file):\n",
    "        \"\"\"\n",
    "        Read in the text and evaluate it.\n",
    "        \n",
    "        Params:\n",
    "            file: string, file name\n",
    "        \"\"\"\n",
    "        with open(file) as f:\n",
    "            text = f.read()\n",
    "            return eval(text)\n",
    "        \n",
    "    # Complete the data file names \n",
    "    name_X = name + '.xdat'\n",
    "    name_y = name + '.ydat'\n",
    "    \n",
    "    # Read in X and y\n",
    "    X = read_eval(name_X)\n",
    "    y = read_eval(name_y)\n",
    "    \n",
    "    # Convert both to np arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Reshape y to make y 2D\n",
    "    y = y.reshape([y.shape[0], -1])\n",
    "    data = np.concatenate((X, y), axis=1)\n",
    "    \n",
    "    # Save to csv\n",
    "    np.savetxt(name + '.csv', data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate('data0')\n",
    "consolidate('data1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.read_csv('alternator_actions.csv')\n",
    "df_s = pd.read_csv('starter_actions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correction_description</th>\n",
       "      <th>\"part_name\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHECK NO START CHARGE BATT ALT NOT CHARGING   ...</td>\n",
       "      <td>\"ALTERNATOR\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check mil light   p3119 stored road test light...</td>\n",
       "      <td>\"ALTERNATOR\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>check mil light   p3119 stored road test light...</td>\n",
       "      <td>\"ALTERNATOR\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST BATTERY AND CHARGED TESTED ALTERNATOR NEE...</td>\n",
       "      <td>\"ALTERNATOR\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIAGNOSIS FOR NO START. SCAN FOR CODE, U0100, ...</td>\n",
       "      <td>\"ALTERNATOR\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              correction_description    \"part_name\"\n",
       "0  CHECK NO START CHARGE BATT ALT NOT CHARGING   ...   \"ALTERNATOR\"\n",
       "1  check mil light   p3119 stored road test light...   \"ALTERNATOR\"\n",
       "2  check mil light   p3119 stored road test light...   \"ALTERNATOR\"\n",
       "3  TEST BATTERY AND CHARGED TESTED ALTERNATOR NEE...   \"ALTERNATOR\"\n",
       "4  DIAGNOSIS FOR NO START. SCAN FOR CODE, U0100, ...   \"ALTERNATOR\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correction_description</th>\n",
       "      <th>\"part_name\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inspected vehicle for clicking and no start co...</td>\n",
       "      <td>\"STARTER\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHECK FOR NO START JUST CLICK CHECKED FOR A NO...</td>\n",
       "      <td>\"STARTER\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INSPECT NO START CHECK CODES-P127A STARTER CON...</td>\n",
       "      <td>\"STARTER\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHECK NO START    OK NOW REMOVE CONNECTIONS OF...</td>\n",
       "      <td>\"STARTER\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7 way connector wiring has melted on exhaust, ...</td>\n",
       "      <td>\"STARTER\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              correction_description  \"part_name\"\n",
       "0  inspected vehicle for clicking and no start co...    \"STARTER\"\n",
       "1  CHECK FOR NO START JUST CLICK CHECKED FOR A NO...    \"STARTER\"\n",
       "2  INSPECT NO START CHECK CODES-P127A STARTER CON...    \"STARTER\"\n",
       "3  CHECK NO START    OK NOW REMOVE CONNECTIONS OF...    \"STARTER\"\n",
       "4  7 way connector wiring has melted on exhaust, ...    \"STARTER\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 2 columns):\n",
      "correction_description    11 non-null object\n",
      " \"part_name\"              11 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 256.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38 entries, 0 to 37\n",
      "Data columns (total 2 columns):\n",
      "correction_description    38 non-null object\n",
      " \"part_name\"              38 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 688.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_s.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Needed for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gather Data\n",
    "To classify text descriptions into two \"labels\" / \"classes\" - alternator and starter, we need to build a Machine Learning model. This model can be trained using a \"training set\". The training set needs to have samples from both classes and the corresponding class labels, so that the model can learn which sample corresponds to which label. Therefore, the step is to combine both datasets into one. Steps:\n",
    "\n",
    "1. Combine datasets into one training set. We can use `Pandas` to read in both datasets and use the `concat` function to combine the datasets.\n",
    "2. Shuffle the dataset so that the dataset can reflect the actual distribution. To shuffle the dataset, we can use `numpy.random` module's `shuffle` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data\n",
    "\n",
    "There are many inconsistencies in the text, for example, the same words can be in lower or uppper case. Also, since Machine Learning algorithms only deal with numeric inputs, our data will need to be converted from text to numeric values first. All these and more are all examples of preprocessing needed before we are able to feed the data in a Machine Learning algorithm. The detailed steps are:\n",
    "\n",
    "1. Convert to lowercase: This can be done with Python's string methods `.lower()`.\n",
    "2. Remove punctuation: This can be done using Regular Expressions(RE).\n",
    "3. Remove common words that are not helpful (stopwords): In the English language, there are many words that are so frequently seen that they are not helpful in the classification, examples include \"the\", \"this\" etc. Both this and the next step can be done using Natrual Language Processing Toolkit (NLTK) or scikit-learn.\n",
    "4. Convert words to root form (stemming): we don't want different forms of the same word to be treated as different words with different meanings, so we need to perform stemming. This way, \"does\" and \"do\" will be the same word. \n",
    "5. Split into separate words (tokenization): we want to transform our text samples to separate words so that they can be represented by numbers. To do that, we first need to split the samples. This can be done with Python's string method `.split`.\n",
    "6. Transform into numerical values (vectorization): For the labels, we can use `LabelEncoder` class from the `sklearn.preprocessing` module to convert to numeric. For the descriptions (features), we can use `scikit-learn`'s `CountVectorizer` or `TfIdfVectorizer`.\n",
    "\n",
    "Once this step is done, we'll have a matrix of numeric values that we can feed into a Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Building and Testing\n",
    "\n",
    "1. Spot-checking: First, I want to spot-check different algorithms to find the most promising algorithms to use for this specific dataset. I'll try different categories of algorithms including:\n",
    " - Linear: Logistic Regression, Linear Discriminant Analysis etc.\n",
    " - Nonlinear: Naive Bayes, XGBoost, K-Nearest Neighbors, Support Vector Machines, Neural Networks etc.\n",
    " - Emsemble: AdaBoost, Random Forest etc.\n",
    "   I'll use `scikit-learn` and XGBoost for this part.\n",
    "2. Model building and tuning: Once I identify several algorithms that are most promising, I'll start building the model using cross-validation. I'll use a 80:20 split for train and validation sets and stratified 10-fold cross-validation. I'll use `scikit-learn`'s `GridSearchCV` and `RandomizedSearchCV` for hyperparameter tuning to find the best model.\n",
    "3. Model testing: Finally, I'll use `pickle` to persist the trained model. Later on, once I have access to a new dataset, I'll be able to load the trained model, instantiate it and make predictions using the model's `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
